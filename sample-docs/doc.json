[ "<h1>Best practices for running cost-optimized| Kubernetes applications on GKE|", "", "<p>This document discusses Google Kubernetes Engine (GKE)", "<s2>\u00a0(/kubernetes-engine)", "<p> features and options,|", "<p>and the best practices for running cost-optimized applications on GKE to take advantage of the|", "<p>elasticity provided by Google Cloud. This document assumes that you are familiar with Kubernetes,| Google Cloud, GKE, and autoscaling.|", "", "<h2>Introduction|", "", "<p>As Kubernetes gains widespread adoption, a growing number of enterprises and platform-as-a-|", "<p>service (PaaS) and software-as-a-service (SaaS) providers are using multi-tenant Kubernetes|", "<p>clusters", "<s2>\u00a0(/kubernetes-engine/docs/concepts/multitenancy-overview)", "<p> for their workloads. This means that a|", "<p>single cluster might be running applications that belong to different teams, departments, customers,|", "<p>or environments. The multi-tenancy provided by Kubernetes lets companies manage a few large| clusters, instead of multiple smaller ones, with bene\u00a6ts such as appropriate resource utilization,|", "<p>simpli\u00a6ed management control, and reduced fragmentation.|", "<p>Over time, some of these companies with fast-growing Kubernetes clusters start to experience a|", "<p>disproportionate increase in cost. This happens because traditional companies that embrace cloud-|", "<p>based solutions like Kubernetes don't have developers and operators with cloud expertise. This lack|", "<p>of cloud readiness leads to applications becoming unstable during autoscaling (for example, tra\u00a8c|", "<p>volatility during a regular period of the day), sudden bursts, or spikes (such as TV commercials or| peak scale events like Black Friday and Cyber Monday). In an attempt to \"\u00a6x\" the problem, these|", "<p>companies tend to over-provision their clusters the way they used to in a non-elastic environment.|", "<p>Over-provisioning results in considerably higher CPU and memory allocation than what applications|", "<p>use for most of the day.|", "<p>This document provides best practices for running cost-optimized Kubernetes workloads on GKE.|", "<p>The following diagram outlines this approach.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 2/33|", "", "<p>The foundation of building cost-optimized applications is spreading the cost-saving culture across|", "<p>teams. Beyond moving cost discussions to the beginning of the development process, this approach|", "<p>forces you to better understand the environment that your applications are running in\u2014in this|", "<p>context, the GKE environment.|", "<p>In order to achieve low cost and application stability, you must correctly set or tune some features|", "<p>and con\u00a6gurations (such as autoscaling, machine types, and region selection). Another important|", "<p>consideration is your workload type because, depending on the workload type and your application's|", "<p>requirements, you must apply different con\u00a6gurations in order to further lower your costs. Finally,|", "<p>you must monitor your spending and create guardrails so that you can enforce best practices early|", "<p>in your development cycle.|", "<p>The following table summarizes the challenges that GKE helps you solve. Although we encourage|", "<p>you to read the whole document, this table presents a map of what's covered.|", "", "<s2>Challenge| Action|", "<s2>I want to look at| easy cost savings| on GKE.|", "<s2>Select the appropriate region\u00a0(#select_the_appropriate_region), sign up for committed-use| discounts\u00a0(#sign_up_for_committed_use_discounts), and use E2 machine types| \u00a0(#e2_machine_types).|", "<s2>I need to| understand my| GKE costs.|", "<s2>Observe your GKE clusters and watch for recommendations| \u00a0(#observe_your_gke_clusters_and_watch_for_recommendations), and enable GKE usage| metering\u00a0(#enable_gke_usage_metering)|", "<s2>I want to make the| most out of GKE| elasticity for my| existing workloads.|", "<s2>Read Horizontal Pod Autoscaler\u00a0(#horizontal_pod_autoscaler), Cluster Autoscaler| \u00a0(#cluster_autoscaler), and understand best practices for Autoscaler and over-provisioning| \u00a0(#autoscaler_and_over-provisioning).|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 3/33|", "", "<s2>Challenge| Action|", "<s2>I want to use the| most e\u00a8cient| machine types.|", "<s2>Choose the right machine type\u00a0(#choose_the_right_machine_type) for your workload.|", "<s2>Many nodes in my| cluster are sitting| idle.|", "<s2>Read best practices for Cluster Autoscaler\u00a0(#cluster_autoscaler).|", "<s2>I need to improve| cost savings in my| batch jobs.|", "<s2>Read best practices for batch workloads\u00a0(#batch_workloads).|", "<s2>I need to improve| cost savings in my| serving workloads.|", "<s2>Read best practices for serving workloads\u00a0(#serving_workloads).|", "<s2>I don't know how to| size my Pod| resource requests.|", "<s2>Use Vertical Pod Autoscaler (VPA)\u00a0(#vertical_pod_autoscaler), but pay attention to mixing| Horizontal Pod Autoscaler (HPA) and VPA\u00a0(#mixing_hpa_and_vpa) best practices.|", "<s2>My applications are| unstable during| autoscaling and| maintenance| activities.|", "<s2>Prepare cloud-based applications for Kubernetes| \u00a0(#prepare_cloud-based_applications_for_kubernetes), and understand how Metrics Server| works and how to monitor it\u00a0(#understand_how_metrics_server_works_and_how_to_monitor_it)| .|", "<s2>How do I make my| developers pay| attention to their| applications'| resource usage?|", "<s2>Spread the cost-saving culture\u00a0(#spread_the_cost-saving_culture), consider using GKE| Enterprise Policy Controller\u00a0(#consider_using_anthos_policy_controller), design your CI/CD| pipeline to enforce cost savings practices| \u00a0(#design_your_cicd_pipeline_to_enforce_cost-saving_practices), and use Kubernetes resource| quotas\u00a0(#use_kubernetes_resource_quotas).|", "<s2>What else should I| consider to further| reduce my| ecosystem costs?|", "<s2>Review small development clusters\u00a0(#review_small_development_clusters), review your logging| and monitoring strategies\u00a0(#review_your_logging_and_monitoring_strategies), and review inter-| region egress tra\u00a8c in regional and multi-zonal clusters| \u00a0(#review_inter-region_egress_tra\u00a8c_in_regional_and_multi-zonal_clusters).|", "", "<h2>GKE cost-optimization features and options|", "", "<p>Cost-optimized Kubernetes applications rely heavily on GKE autoscaling. To balance cost, reliability,|", "<p>and scaling performance on GKE, you must understand how autoscaling works and what options|", "<p>you have. This section discusses GKE autoscaling and other useful cost-optimized con\u00a6gurations|", "<p>for both serving and batch workloads.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 4/33|", "", "<h3>Fine-tune GKE autoscaling|", "", "<p>Autoscaling is the strategy GKE uses to let Google Cloud customers pay only for what they need by| minimizing infrastructure uptime. In other words, autoscaling saves costs by 1) making workloads|", "<p>and their underlying infrastructure start before demand increases, and 2) shutting them down when|", "<p>demand decreases.|", "<p>The following diagram illustrates this concept. In Kubernetes, your workloads are containerized|", "<p>applications that are running inside Pods", "<s2> \u00a0(https://kubernetes.io/docs/concepts/workloads/pods/pod/)", "<p>, and|", "<p>the underlying infrastructure, which is composed of a set of Nodes, must provide enough computing|", "<p>capacity to run the workloads.|", "<p>As the following diagram shows, this environment has four scalability dimensions. The workload|", "<p>and infrastructure can scale horizontally by adding and removing Pods or Nodes, and they can scale|", "<p>vertically by increasing and decreasing Pod or Node size.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 5/33|", "", "<p>GKE handles these autoscaling scenarios by using features like the following:|", "<p>Horizontal Pod Autoscaler (HPA)", "<s2>\u00a0(#horizontal_pod_autoscaler)", "<p>, for adding and removing Pods|", "<p>based on utilization metrics.|", "<p>Vertical Pod Autoscaler (VPA)", "<s2>\u00a0(#vertical_pod_autoscaler)", "<p>, for sizing your Pods.|", "<p>Cluster Autoscaler", "<s2>\u00a0(#cluster_autoscaler)", "<p>, for adding and removing Nodes based on the scheduled|", "<p>workload.|", "<p>Node auto-provisioning", "<s2>\u00a0(/kubernetes-engine/docs/how-to/node-auto-provisioning)", "<p>, for dynamically|", "<p>creating new node pools with nodes that match the needs of users' Pods.|", "<p>The following diagram illustrates these scenarios.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 6/33|", "", "<p>The remainder of this section discusses these GKE autoscaling capabilities in more detail and|", "<p>covers other useful cost-optimized con\u00a6gurations for both serving and batch workloads.|", "<p>Horizontal Pod Autoscaler|", "<p>Horizontal Pod Autoscaler", "<s2> \u00a0(https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)|", "", "<p>(HPA) is meant for scaling applications that are running in Pods based on metrics that express load.|", "<p>You can con\u00a6gure either CPU utilization or other custom metrics (for example, requests per second).|", "<p>In short, HPA adds and deletes Pods replicas, and it is best suited for stateless workers that can|", "<p>spin up quickly", "<s2>\u00a0(#make_sure_your_container_is_as_lean_as_possible)", "<p> to react to usage spikes, and shut|", "<p>down gracefully|", "", "<s2>\u00a0(#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectations)", "<p> to avoid| workload instability.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 7/33|", "", "<p>As the preceding image shows, HPA requires a target utilization threshold, expressed in percentage,|", "<p>which lets you customize when to automatically trigger scaling. In this example, the target CPU|", "<p>utilization is 70%. That means your workload has a 30% CPU buffer for handling requests while new|", "<p>replicas are spinning up. A small buffer prevents early scale-ups, but it can overload your application|", "<p>during spikes. However, a large buffer causes resource waste, increasing your costs. The exact|", "<p>target is application speci\u00a6c, and you must consider the buffer size to be enough for handling|", "<p>requests for two or three minutes during a spike. Even if you guarantee that your application can|", "<p>start up in a matter of seconds, this extra time is required when Cluster Autoscaler|", "", "<s2>\u00a0(#cluster_autoscaler)", "<p> adds new nodes to your cluster or when Pods are throttled due to lack of|", "<p>resources.|", "<p>The following are best practices for enabling HPA in your application:|", "<p>Size your application correctly by setting appropriate resource requests and limits.|", "", "<s2>\u00a0(#set_appropriate_resource_requests_and_limits)|", "", "<p>Set your target utilization to reserve a buffer that can handle requests during a spike.|", "<p>Make sure your application starts as quickly as possible|", "", "<s2>\u00a0(#make_sure_your_container_is_as_lean_as_possible)", "<p> and shuts down according to Kubernetes|", "<p>expectations|", "", "<s2>\u00a0(#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectations)", "<p>.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 8/33|", "", "<p>Set meaningful readiness and liveness probes|", "", "<s2>\u00a0(#set_meaningful_readiness_and_liveness_probes_for_your_application)", "<p>.|", "<p>Make sure that your Metrics Server is always up and running|", "", "<s2>\u00a0(#understand_how_metrics_server_works_and_how_to_monitor_it)", "<p>.|", "<p>Inform clients of your application that they must consider implementing exponential retries|", "", "<s2>\u00a0(#consider_using_retries_with_exponential_backoff)", "<p> for handling transient issues.|", "<p>For more information, see Con\u00a6guring a Horizontal Pod Autoscaler|", "", "<s2>\u00a0(/kubernetes-engine/docs/how-to/horizontal-pod-autoscaling)", "<p>.|", "<p>Vertical Pod Autoscaler|", "<p>Unlike HPA, which adds and deletes Pod replicas for rapidly reacting to usage spikes, Vertical Pod|", "<p>Autoscaler", "<s2>\u00a0(/kubernetes-engine/docs/concepts/verticalpodautoscaler)", "<p> (VPA) observes Pods over time and|", "<p>gradually \u00a6nds the optimal CPU and memory resources required by the Pods. Setting the right|", "<p>resources is important for stability and cost e\u00a8ciency. If your Pod resources are too small, your|", "<p>application can either be throttled or it can fail due to out-of-memory errors. If your resources are too|", "<p>large, you have waste and, therefore, larger bills. VPA is meant for stateless and stateful workloads|", "<p>not handled by HPA or when you don't know the proper Pod resource requests.|", "<p>As the preceding image shows, VPA detects that the Pod is consistently running at its limits and|", "<p>recreates the Pod with larger resources. The opposite also happens when the Pod is consistently|", "<p>underutilized\u2014a scale-down is triggered.|", "<p>VPA can work in three different modes:|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 9/33|", "", "<p>Off . In this mode, also known as  recommendation mode , VPA does not apply any change to|", "<p>your Pod. The recommendations are calculated and can be inspected in the VPA object.|", "<p>Initial : VPA assigns resource requests only at Pod creation and never changes them later.|", "<p>Auto : VPA updates CPU and memory requests during the life of a Pod. That means, the Pod is|", "<p>deleted, CPU and memory are adjusted, and then a new Pod is started.|", "<p>If you plan to use VPA, the best practice is to start with the  Off  mode for pulling VPA|", "<p>recommendations. Make sure it's running for 24 hours, ideally one week or more, before pulling|", "<p>recommendations. Then, only when you feel con\u00a6dent, consider switching to either  Initial  or  Auto|", "<p>mode.|", "<p>Follow these best practices for enabling VPA, either in  Initial  or  Auto  mode, in your application:|", "<p>Don't use VPA either  Initial  or  Auto  mode if you need to handle sudden spikes in tra\u00a8c. Use|", "<p>HPA", "<s2>\u00a0(#horizontal_pod_autoscaler)", "<p> instead.|", "<p>Make sure your application can grow vertically|", "", "<s2>\u00a0(#make_sure_your_application_can_grow_vertically_and_horizontally)", "<p>.|", "<p>Set minimum and maximum container sizes in the VPA objects to avoid the autoscaler making|", "<p>signi\u00a6cant changes when your application is not receiving tra\u00a8c.|", "<p>Don't make abrupt changes, such as dropping the Pod's replicas from 30 to 5 all at once. This|", "<p>kind of change requires a new deployment, new label set, and new VPA object.|", "<p>Make sure your application starts as quickly as possible|", "", "<s2>\u00a0(#make_sure_your_container_is_as_lean_as_possible)", "<p> and shuts down according to Kubernetes|", "<p>expectations|", "", "<s2>\u00a0(#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectations)", "<p>.|", "<p>Set meaningful readiness and liveness probes|", "", "<s2>\u00a0(#set_meaningful_readiness_and_liveness_probes_for_your_application)", "<p>.|", "<p>Make sure that your Metrics Server is always up and running|", "", "<s2>\u00a0(#understand_how_metrics_server_works_and_how_to_monitor_it)", "<p>.|", "<p>Inform clients of your application that they must consider implementing exponential retries|", "", "<s2>\u00a0(#consider_using_retries_with_exponential_backoff)", "<p> for handling transient issues.|", "<p>Consider using node auto-provisioning along with VPA|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/verticalpodautoscaler#vertical_pod_autoscaling_in_auto_mode)", "<p> so|", "<p>that if a Pod gets large enough to \u00a6t into existing machine types, Cluster Autoscaler provisions|", "<p>larger machines to \u00a6t the new Pod.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 10/33|", "", "<p>Whether you are considering using  Auto  mode, make sure you also follow these practices:|", "<p>Make sure your application can be restarted while receiving tra\u00a8c.|", "<p>Add Pod Disruption Budget (PDB)", "<s2>\u00a0(#add-pod_disruption_budget-to-your-application)", "<p> to control how|", "<p>many Pods can be taken down at the same time.|", "<p>For more information, see Con\u00a6guring Vertical Pod Autoscaling|", "", "<s2>\u00a0(/kubernetes-engine/docs/how-to/vertical-pod-autoscaling)", "<p>.|", "<p>Mixing HPA and VPA|", "<p>The o\u00a8cial recommendation|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/verticalpodautoscaler#limitations_for_vertical_pod_autoscaling)", "<p> is that you|", "<p>must not mix VPA and HPA on either CPU or memory. However, you can mix them safely when using|", "<p>recommendation mode in VPA or custom metrics in HPA\u2014for example, requests per second. When|", "<p>mixing VPA with HPA, make sure your deployments are receiving enough tra\u00a8c\u2014meaning, they are|", "<p>consistently running above the HPA min-replicas. This lets VPA understand your Pod's resource|", "<p>needs.|", "<p>For more information about VPA limitations, see Limitations for Vertical Pod autoscaling|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/verticalpodautoscaler)", "<p>.|", "<p>Cluster Autoscaler|", "<p>Cluster Autoscaler", "<s2>\u00a0(/kubernetes-engine/docs/concepts/cluster-autoscaler)", "<p> (CA) automatically resizes the|", "<p>underlying computer infrastructure. CA provides nodes for Pods that don't have a place to run in the|", "<p>cluster and removes under-utilized nodes. CA is optimized for the cost of infrastructure. In other|", "<p>words, if there are two or more node types in the cluster, CA chooses the least expensive one that|", "<p>\u00a6ts the given demand.|", "<p>Unlike HPA and VPA, CA doesn't depend on load metrics. Instead, it's based on scheduling|", "<p>simulation and declared Pod requests. It's a best practice to enable CA whenever you are using|", "<p>either HPA or VPA. This practice ensures that if your Pod autoscalers determine that you need more|", "<p>capacity, your underlying infrastructure grows accordingly.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 11/33|", "", "<p>As these diagrams show, CA automatically adds and removes compute capacity to handle tra\u00a8c|", "<p>spikes and save you money when your customers are sleeping. It is a best practice to de\u00a6ne Pod| Disruption Budget", "<s2>\u00a0(#add-pod_disruption_budget-to-your-application)", "<p> (PDB) for all your applications. It is|", "<p>particularly important at the CA scale-down phase when PDB controls the number of replicas that|", "<p>can be taken down at one time.|", "<p>Certain Pods cannot be restarted by any autoscaler|", "", "<s2>\u00a0(https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-| prevent-ca-from-removing-a-node)|", "", "<p>when they cause some temporary disruption, so the node they run on can't be deleted. For example,|", "<p>system Pods (such as ", "<s1>metrics-server", "<p> and ", "<s1>kube-dns", "<p>), and Pods using local storage won't be|", "<p>restarted. However, you can change this behavior by de\u00a6ning PDBs|", "", "<s2>\u00a0(https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-to-set-pdbs-to-enable-| ca-to-move-kube-system-pods)|", "", "<p>for these system Pods and by setting ", "<s1>\"cluster-autoscaler.kubernetes.io/safe-to-evict\":|", "<s1>\"true\"|", "", "<s2>\u00a0(https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-| prevent-ca-from-removing-a-node)|", "", "<p>annotation for Pods using local storage that are safe for the autoscaler to restart. Moreover,|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 12/33|", "", "<p>consider running long-lived Pods that can't be restarted on a separate node pool|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/node-pools)", "<p>, so they don't block scale-down of other nodes. Finally,|", "<p>learn how to analyze CA events in the logs", "<s2>\u00a0(/kubernetes-engine/docs/how-to/cluster-autoscaler-visibility)", "<p> to|", "<p>understand why a particular scaling activity didn't happen as expected.|", "<p>If your workloads are resilient to nodes restarting inadvertently and to capacity losses|", "", "<s2>\u00a0(#prepare_your_environment_to_\u00a6t_your_workload_type)", "<p>, you can save more money by creating a cluster or|", "<p>node pool with preemptible VMs", "<s2>\u00a0(/kubernetes-engine/docs/how-to/preemptible-vms#create)", "<p>. For CA to|", "<p>work as expected, Pod resource requests need to be large enough for the Pod to function normally|", "", "<s2>\u00a0(#set_appropriate_resource_requests_and_limits)", "<p>. If resource requests are too small, nodes might not|", "<p>have enough resources and your Pods might crash or have troubles during runtime.|", "<p>The following is a summary of the best practices for enabling Cluster Autoscaler in your cluster:|", "<p>Use either HPA or VPA to autoscale your workloads.|", "<p>Make sure you are following the best practices described in the chosen Pod autoscaler.|", "<p>Size your application correctly by setting appropriate resource requests and limits|", "", "<s2>\u00a0(#set_appropriate_resource_requests_and_limits)", "<p> or use VPA.|", "<p>De\u00a6ne a PDB for your applications.|", "<p>De\u00a6ne PDB for system Pods that might block your scale-down. For example, ", "<s1>kube-dns", "<p>. To|", "<p>avoid temporary disruption in your cluster, don't set PDB for system Pods that have only 1|", "<p>replica (such as ", "<s1>metrics-server", "<p>).|", "<p>Run short-lived Pods and Pods that can be restarted in separate node pools, so that long-lived|", "<p>Pods don't block their scale-down.|", "<p>Avoid over-provisioning by con\u00a6guring idle nodes in your cluster. For that, you must know your|", "<p>minimum capacity\u2014for many companies it's during the night\u2014and set the minimum number of|", "<p>nodes in your node pools to support that capacity.|", "<p>If you need extra capacity to handle requests during spikes, use pause Pods, which are|", "<p>discussed in Autoscaler and over-provisioning", "<s2>\u00a0(#autoscaler_and_over-provisioning)", "<p>.|", "<p>For more information, see Autoscaling a cluster", "<s2>\u00a0(/kubernetes-engine/docs/how-to/cluster-autoscaler)", "<p>.|", "<p>Node auto-provisioning|", "<p>Node auto-provisioning", "<s2>\u00a0(/kubernetes-engine/docs/how-to/node-auto-provisioning)", "<p> (NAP) is a mechanism|", "<p>of Cluster Autoscaler that automatically adds new node pools|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/node-pools)", "<p> in addition to managing their size on the user's behalf.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 13/33|", "", "<p>Without node auto-provisioning, GKE considers starting new nodes only from the set of user-created| node pools. With node auto-provisioning, GKE can create and delete new node pools automatically.|", "<p>Node auto-provisioning tends to reduce resource waste by dynamically creating node pools that best|", "<p>\u00a6t with the scheduled workloads. However, the autoscale latency can be slightly higher when new| node pools need to be created. If your workloads are resilient to nodes restarting inadvertently and|", "<p>to capacity losses", "<s2>\u00a0(#prepare_your_environment_to_\u00a6t_your_workload_type)", "<p>, you can further lower costs by| con\u00a6guring a preemptible VM's toleration in your Pod|", "", "<s2>\u00a0(/kubernetes-engine/docs/how-to/node-auto-provisioning#support_for_preemptible_vms)", "<p>.|", "<p>The following are best practices for enabling node auto-provisioning:|", "<p>Follow all the best practice of Cluster Autoscaler.|", "<p>Set minimum and maximum resources sizes to avoid NAP making signi\u00a6cant changes in your| cluster when your application is not receiving tra\u00a8c.|", "<p>When using Horizontal Pod Autoscaler for serving workloads, consider reserving a slightly|", "<p>larger target utilization buffer because NAP might increase autoscaling latency in some cases.|", "<p>For more information, see Using node auto-provisioning|", "", "<s2>\u00a0(/kubernetes-engine/docs/how-to/node-auto-provisioning)", "<p> and Unsupported features|", "", "<s2>\u00a0(/kubernetes-engine/docs/how-to/node-auto-provisioning#unsupported_features)", "<p>.|", "<p>Autoscaler and over-provisioning|", "<p>In order to control your costs, we strongly recommend that you enable autoscaler according to the| previous sections. No one con\u00a6guration \u00a6ts all possible scenarios, so you must \u00a6ne-tune the|", "<p>settings for your workload to ensure that autoscalers respond correctly to increases in tra\u00a8c.|", "<p>However, as noted in the Horizontal Pod Autoscaler", "<s2>\u00a0(#horizontal_pod_autoscaler)", "<p> section, scale-ups| might take some time due to infrastructure provisioning. To visualize this difference in time and|", "<p>possible scale-up scenarios, consider the following image.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 14/33|", "", "<p>When your cluster has enough room for deploying new Pods, one of the  Workload scale-up|", "<p>scenarios  is triggered. Meaning, if an existing node never deployed your application, it must| download its container images before starting the Pod (scenario 1). However, if the same node must|", "<p>start a new Pod replica of your application, the total scale-up time decreases because no image| download is required (scenario 2).|", "<p>When your cluster doesn't have enough room for deploying new Pods, one of the  Infrastructure and| Workload scale-up scenarios  is triggered. This means that Cluster Autoscaler must provision new|", "<p>nodes and start the required software before approaching your application (scenario 1). If you use| node auto-provisioning, depending on the workload scheduled, new node pools might be required. In|", "<p>this situation, the total scale-up time increases because Cluster Autoscaler has to provision nodes| and node pools (scenario 2).|", "<p>For scenarios where new infrastructure is required, don't squeeze your cluster too much\u2014meaning,|", "<p>you must over-provision but only for reserving the necessary buffer to handle the expected peak| requests during scale-ups.|", "<p>There are two main strategies for this kind of over-provisioning:|", "<p>Fine-tune the HPA utilization target . The following equation is a simple and safe way to \u00a6nd a| good CPU target:|", "<p>(1 -  buff )/(1 +  perc )|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 15/33|", "", "<h2>\ue838|", "", "<p>buff  is a safety buffer that you can set to avoid reaching 100% CPU. This variable is|", "<p>useful because reaching 100% CPU means that the latency of request processing is| much higher than usual.|", "<p>perc  is the percentage of tra\u00a8c growth you expect in two or three minutes.|", "<p>For example, if you expect a growth of 30% in your requests and you want to avoid reaching| 100% of CPU by de\u00a6ning a 10% safety buffer, your formula would look like this:|", "<p>(1 -  0.1 )/(1 +  0.3 ) = 0.69|", "", "<s2>Note:  If you have large nodes and small applications whose tra\u00a8c is consistently below what is requested,|", "<s2>then you can get more aggressive (by reducing or removing the  buff  variable). You can be more aggressive|", "<s2>because there are likely some free CPU cycles on the machine, so in some cases, it's safe for Pods to pass|", "<s2>100% utilization.|", "", "<p>Con\u00a6gure pause Pods . There is no way to con\u00a6gure Cluster Autoscaler to spin up nodes| upfront. Instead, you can set an HPA utilization target to provide a buffer to help handle spikes|", "<p>in load. However, if you expect large bursts, setting a small HPA utilization target might not be| enough or might become too expensive.|", "<p>An alternative solution for this problem is to use pause Pods|", "", "<s2>\u00a0(https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-con\u00a6gure-| overprovisioning-with-cluster-autoscaler)|", "", "<p>. Pause Pods are low-priority deployments that do nothing but reserve room in your cluster.|", "<p>Whenever a high-priority Pod is scheduled, pause Pods get evicted and the high-priority Pod| immediately takes their place. The evicted pause Pods are then rescheduled, and if there is no|", "<p>room in the cluster, Cluster Autoscaler spins up new nodes for \u00a6tting them. It's a best practice| to have only a single pause Pod per node. For example, if you are using 4 CPU nodes, con\u00a6gure|", "<p>the pause Pods' CPU request with around 3200m.|", "", "<h3>Choose the right machine type|", "", "<p>Beyond autoscaling, other con\u00a6gurations can help you run cost-optimized kubernetes applications| on GKE. This section discusses choosing the right machine type.|", "<p>Preemptible VMs|", "<p>Preemptible VMs", "<s2>\u00a0(/kubernetes-engine/docs/how-to/preemptible-vms)", "<p> (PVMs) are Compute Engine VM|", "<p>instances that last a maximum of 24 hours and provide no availability guarantees. PVMs are up to| 80% cheaper", "<s2>\u00a0(/compute/all-pricing)", "<p> than standard Compute Engine VMs, but we recommend that you|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 16/33|", "", "<p>use them with caution on GKE clusters. PVMs on GKE are best suited for running batch or fault-| tolerant jobs that are less sensitive to the ephemeral, non-guaranteed nature of PVMs. Stateful and|", "<p>serving workloads must not use PVMs unless you prepare your system and architecture to handle| PVMs' constraints.|", "<p>Whatever the workload type, you must pay attention to the following constraints:|", "<p>Pod Disruption Budget might not be respected because preemptible nodes can shut down| inadvertently.|", "<p>There is no guarantee that your Pods will shut down gracefully|", "", "<s2>\u00a0(#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectations)", "<p> once|", "<p>node preemption ignores the Pod grace period|", "", "<s2>\u00a0(https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods)", "<p>.|", "<p>It might take several minutes for GKE to detect that the node was preempted and that the Pods|", "<p>are no longer running, which delays rescheduling the Pods to a new node.|", "<p>In order to mitigate these constraints, you can deploy in your cluster a community Node Termination| Event Handler", "<s2> \u00a0(https://github.com/GoogleCloudPlatform/k8s-node-termination-handler)", "<p> project (important:|", "<p>this is not an o\u00a8cial Google project) that provides an adapter for translating Compute Engine node| termination events to graceful Pod terminations in Kubernetes. This community project does not|", "<p>reliably solve all the PVMs' constraints once Pod Disruption Budgets can still be disrespected.| Therefore, pods can take a little longer to be rescheduled.|", "<p>Finally, PVMs have no guaranteed availability, meaning that they can stock out easily in some| regions. To overcome this limitation, we recommend that you set a backup node pool without PVMs.| Cluster Autoscaler gives preference to PVMs because it is optimized for infrastructure cost.|", "<p>For more information, see Running preemptible VMs on GKE|", "", "<s2>\u00a0(/kubernetes-engine/docs/how-to/preemptible-vms)", "<p> and Run web applications on GKE using cost-|", "<p>optimized Spot VMs|", "", "<s2>\u00a0(/kubernetes-engine/docs/archive/run-web-applications-on-gke-using-cost-optimized-spot-vms-and-tra\u00a8c-| director)|", "<p>.|", "<p>E2 machine types|", "<p>E2 machine types", "<s2>\u00a0(/blog/products/compute/google-compute-engine-gets-new-e2-vm-machine-types)", "<p> (E2| VMs) are cost-optimized VMs that offer you 31% savings compared to N1 machine types|", "", "<s2>\u00a0(/compute/docs/machine-types#n1_machine_types)", "<p>. E2 VMs are suitable for a broad range of workloads,| including web servers, microservices, business-critical applications, small-to-medium sized|", "<p>databases, and development environments.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 17/33|", "", "<p>For more information about E2 VMs and how they compare with other Google Cloud machine types,| see Performance-driven dynamic resource management in E2 VMs|", "", "<s2>\u00a0(/blog/products/compute/understanding-dynamic-resource-management-in-e2-vms)", "<p> and Machine types|", "", "<s2>\u00a0(/compute/docs/machine-types)", "<p>.|", "", "<h3>Select the appropriate region|", "", "<p>When cost is a constraint, where you run your GKE clusters matters. Due to many factors, cost varies| per computing region", "<s2>\u00a0(/compute/all-pricing)", "<p>. So make sure you are running your workload in the least|", "<p>expensive option but where latency doesn't affect your customer. If your workload requires copying| data from one region to another\u2014for example, to run a batch job\u2014you must also consider the cost of|", "<p>moving this data.|", "<p>For more information on how to choose the right region, see Best practices for Compute Engine| regions selection", "<s2>\u00a0(/solutions/best-practices-compute-engine-region-selection)", "<p>.|", "", "<h3>Sign up for commi\u008fed-use discounts|", "", "<p>If you intend to stay with Google Cloud for a few years, we strongly recommend that you purchase| committed-use discounts", "<s2>\u00a0(/compute/docs/instances/signing-up-committed-use-discounts)", "<p> in return for|", "<p>deeply discounted prices for VM usage. When you sign a committed-use contract, you purchase| compute resources at a discounted price (up to 70% discount) in return for committing to paying for|", "<p>those resources for one year or three years. If you are unsure about how much resource to commit,| look at your minimum computing usage\u2014for example, during nighttime\u2014and commit the payment|", "<p>for that amount.|", "<p>For more information about committed-use prices for different machine types, see VM instances| pricing", "<s2>\u00a0(/compute/all-pricing)", "<p>.|", "", "<h3>Review small development clusters|", "", "<p>For small development clusters where you need to minimize costs, consider using Autopilot|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/autopilot-overview)", "<p> clusters. With clusters in this mode of operation,|", "<p>you aren't charged for system Pods, operating system costs, or unscheduled workloads.|", "", "<h3>Review your logging and monitoring strategies|", "", "<p>If you use Cloud Logging", "<s2>\u00a0(/logging?hl=pt-br)", "<p> and Cloud Monitoring", "<s2>\u00a0(/monitoring)", "<p> to provide observability|", "<p>into your applications and infrastructure, you are paying only for what you use. However, the more| your infrastructure and applications log, and the longer you keep those logs, the more you pay for|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 18/33|", "", "<p>them. Similarly, the more external and custom metrics you have, the higher your costs. Review your|", "<p>logging and monitoring strategies according to Cost optimization for Cloud Logging, Cloud| Monitoring, and Application Performance Management", "<s2>\u00a0(/solutions/stackdriver-cost-optimization)", "<p>.|", "", "<h3>Review inter-region egress tra\u0083c in regional and multi-zonal clusters|", "", "<p>The types of available GKE clusters are single-zone|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/types-of-clusters?hl=fa#single-zone_clusters)", "<p>, multi-zonal|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/types-of-clusters?hl=fa#multi-zonal_clusters)", "<p>, and regional|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/types-of-clusters?hl=fa#regional_clusters)", "<p>. Because of the high|", "<p>availability of nodes across zones, regional and multi-zonal clusters are well suited for production| environments. However, you are charged by the egress tra\u00a8c between zones|", "", "<s2>\u00a0(/vpc/network-pricing#general)", "<p>. For production environments, we recommend that you monitor the| tra\u00a8c load across zones and improve your APIs to minimize it. Also consider using inter-pod a\u00a8nity|", "<p>and anti-a\u00a8nity|", "", "<s2>\u00a0(https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-a\u00a8nity-and-anti-a\u00a8nity)|", "", "<p>con\u00a6gurations to colocate dependent Pods from different services in the same nodes or in the same| availability zone to minimize costs and network latency between them. The suggested way to|", "<p>monitor this tra\u00a8c is to enable GKE usage metering", "<s2>\u00a0(#enable_gke_usage_metering)", "<p> and its network| egress agent", "<s2>\u00a0(/kubernetes-engine/docs/how-to/cluster-usage-metering#enable-network-egress-metering)", "<p>,|", "<p>which is disabled by default.|", "<p>For non-production environments, the best practice for cost saving is to deploy single-zone clusters.|", "", "<h3>Prepare your environment to \u0089t your workload type|", "", "<p>Enterprises have different cost and availability requirements. Their workloads can be divided into|", "<p>serving workloads, which must respond quickly to bursts or spikes, and batch workloads, which are| concerned with eventual work to be done. Serving workloads require a small scale-up latency; batch|", "<p>workloads are more tolerant to latency. The different expectations for these workload types make| choosing different cost-saving methods more \u00a7exible.|", "<p>Batch workloads|", "<p>Because batch workloads are concerned with eventual work, they allow for cost saving on GKE|", "<p>because the workloads are commonly tolerant to some latency at job startup time. This tolerance| gives Cluster Autoscaler space to spin up new nodes only when jobs are scheduled and take them|", "<p>down when the jobs are \u00a6nished.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 19/33|", "", "<p>The \u00a6rst recommended practice is to separate batch workloads in different node pools|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/node-pools)", "<p> by using labels and selectors|", "", "<s2>\u00a0(https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)", "<p>, and by using taints and|", "<p>tolerations", "<s2> \u00a0(https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)", "<p>. The rationale is| the following:|", "<p>Cluster Autoscaler can delete empty nodes faster when it doesn't need to restart pods. As|", "<p>batch jobs \u00a6nish, the cluster speeds up the scale-down process if the workload is running on| dedicated nodes that are now empty. To further improve the speed of scale-downs, consider|", "<p>con\u00a6guring CA's optimize-utilization pro\u00a6le|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_pro\u00a6les)", "<p>.|", "<p>Some Pods cannot be restarted, so they permanently block the scale-down of their nodes.|", "<p>These Pods, which include the system Pods, must run on different node pools so that they| don't affect scale-down.|", "<p>The second recommended practice is to use node auto-provisioning", "<s2>\u00a0(#node-auto-provisioning)", "<p> to|", "<p>automatically create dedicated node pools for jobs with a matching taint or toleration. This way, you| can separate many different workloads without having to set up all those different node pools.|", "<p>We recommend that you use preemptible VMs only if you run fault-tolerant jobs that are less|", "<p>sensitive to the ephemeral, non-guaranteed nature of preemptible VMs.|", "<p>For more information about how to set up an environment that follows these practices, see the|", "<p>Optimizing resource usage in a multi-tenant GKE cluster using node auto-provisioning|", "", "<s2>\u00a0(/solutions/optimizing-resources-in-multi-tenant-gke-clusters-with-auto-provisioning)", "<p> tutorial.|", "<p>Serving workloads|", "<p>Unlike batch workloads, serving workloads must respond as quickly as possible to bursts or spikes.|", "<p>These sudden increases in tra\u00a8c might result from many factors, for example, TV commercials,| peak-scale events like Black Friday, or breaking news. Your application must be prepared to handle|", "<p>them.|", "<p>Problems in handling such spikes are commonly related to one or more of the following reasons:|", "<p>Applications not being ready to run on Kubernetes\u2014for example, apps with large image sizes,| slow startup times, or non-optimal Kubernetes con\u00a6gurations.|", "<p>Applications depending on infrastructure that takes time to be provisioned, like GPUs.|", "<p>Autoscalers and over-provisioning", "<s2>\u00a0(#autoscaler_and_over-provisioning)", "<p> not being appropriately set.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 20/33|", "", "<h2>Prepare cloud-based Kubernetes applications|", "", "<p>Some of the best practices in this section can save money by themselves. However, because most| of these practices are intended to make your application work reliably with autoscalers, we strongly|", "<p>recommend that you implement them.|", "", "<h3>Understand your application capacity|", "", "<p>When you plan for application capacity, know how many concurrent requests your application can|", "<p>handle, how much CPU and memory it requires, and how it responds under heavy load. Most teams| don't know these capacities, so we recommend that you test how your application behaves under|", "<p>pressure. Try isolating a single application Pod replica with autoscaling off, and then execute the| tests simulating a real usage load. This helps you understand your per-Pod capacity. We then|", "<p>recommend con\u00a6guring your Cluster Autoscaler, resource requests and limits, and either HPA or| VPA. Then stress your application again, but with more strength to simulate sudden bursts or spikes.|", "<p>Ideally, to eliminate latency concerns, these tests must run from the same region or zone that the|", "<p>application is running on Google Cloud. You can use the tool of your choice for these tests, whether| it's a homemade script or a more advanced performance tool, like Apache Benchmark|", "", "<s2>\u00a0(https://httpd.apache.org/docs/2.4/programs/ab.html)", "<p>, JMeter", "<s2> \u00a0(https://jmeter.apache.org/)", "<p>, or Locust|", "", "<s2>\u00a0(https://locust.io/)", "<p>.|", "<p>For an example of how you can perform your tests, see Distributed load testing using Google|", "<p>Kubernetes Engine", "<s2>\u00a0(/solutions/distributed-load-testing-using-gke)", "<p>.|", "", "<h3>Make sure your application can grow vertically and horizontally|", "", "<p>Ensure that your application can grow and shrink. This means you can choose to handle tra\u00a8c|", "<p>increases either by adding more CPU and memory or adding more Pod replicas. This gives you the| \u00a7exibility to experiment what \u00a6ts your application better, whether that's a different autoscaler setup|", "<p>or a different node size. Unfortunately, some applications are single threaded or limited by a \u00a6xed| number of workers or subprocesses that make this experiment impossible without a complete|", "<p>refactoring of their architecture.|", "", "<h3>Set appropriate resource requests and limits|", "", "<p>By understanding your application capacity, you can determine what to con\u00a6gure in your container|", "<p>resources. Resources", "<s2> \u00a0(https://kubernetes.io/docs/concepts/con\u00a6guration/manage-resources-containers/)", "<p> in| Kubernetes are mainly de\u00a6ned as CPU and memory (RAM). You con\u00a6gure CPU or memory as the|", "<p>amount required to run your application by using the request|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 21/33|", "", "<s1>spec.containers[].resources.requests.<cpu|memory>", "<p>, and you con\u00a6gure the cap by using the| request ", "<s1>spec.containers[].resources.limits.<cpu|memory>", "<p>.|", "<p>When you've correctly set resource requests, Kubernetes scheduler can use them to decide which|", "<p>node to place your Pod on. This guarantees that Pods are being placed in nodes that can make them| function normally, so you experience better stability and reduced resource waste. Moreover, de\u00a6ning|", "<p>resource limits helps ensure that these applications never use all available underlying infrastructure| provided by computing nodes.|", "<p>A good practice for setting your container resources is to use the same amount of memory for| requests and limits, and a larger or unbounded CPU limit. Take the following deployment as an|", "<p>example:|", "<p>The reasoning for the preceding pattern is founded on how Kubernetes out-of-resource handling|", "", "<s2>\u00a0(https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/)", "<p> works. Brie\u00a7y, when computer| resources are exhausted, nodes become unstable. To avoid this situation, ", "<s1>kubelet|", "", "<s2>\u00a0(https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)", "<p> monitors and prevents total| starvation of these resources by ranking the resource-hungry Pods|", "", "<s2>\u00a0(https://github.com/kubernetes/design-proposals-archive/blob/main/node/resource-qos.md#qos-classes)", "<p>.|", "", "<s2>apiVersion: apps/v1| kind: Deployment| metadata:| \u00a0 name: wordpress| spec:| \u00a0 replicas: 1| \u00a0 selector:| \u00a0 \u00a0 matchLabels:| \u00a0 \u00a0 \u00a0 app: wp|", "<s2>template:|", "<s2>\u00a0 \u00a0 metadata:| \u00a0 \u00a0 \u00a0 labels:| \u00a0 \u00a0 \u00a0 \u00a0 app: wp| \u00a0 \u00a0 spec:| \u00a0 \u00a0 \u00a0 containers:|", "<s2>- name: wp|", "<s2>\u00a0 \u00a0 image: wordpress| \u00a0 \u00a0 resources:| \u00a0 \u00a0 \u00a0 requests:| \u00a0 \u00a0 \u00a0 \u00a0 memory: \"128Mi\"| \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"250m\"| \u00a0 \u00a0 \u00a0 limits:| \u00a0 \u00a0 \u00a0 \u00a0 memory: \"128Mi\"|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 22/33|", "", "<p>When the CPU is contended, these Pods can be throttled down to its requests. However, because| memory is an incompressible resource, when memory is exhausted, the Pod needs to be taken|", "<p>down. To avoid having Pods taken down\u2014and consequently, destabilizing your environment\u2014you| must set requested memory to the memory limit.|", "<p>You can also use VPA in recommendation mode to help you determine CPU and memory usage for a|", "<p>given application. Because VPA provides such recommendations based on your application usage,| we recommend that you enable it in a production-like environment to face real tra\u00a8c. VPA status|", "<p>then generates a report with the suggested resource requests and limits, which you can statically| specify in your deployment manifest. If your application already de\u00a6nes HPA, see Mixing HPA and|", "<p>VPA", "<s2>\u00a0(#mixing_hpa_and_vpa)", "<p>.|", "", "<h3>Make sure your container is as lean as possible|", "", "<p>When you run applications in containers, it's important to follow some practices for building those|", "<p>containers", "<s2>\u00a0(/solutions/best-practices-for-building-containers#build-the-smallest-image-possible)", "<p>. When| running those containers on Kubernetes, some of these practices are even more important because|", "<p>your application can start and stop at any moment. This section focuses mainly on the following two| practices:|", "<p>Have the smallest image possible . It's a best practice to have small images because every|", "<p>time Cluster Autoscaler provisions a new node for your cluster, the node must download the| images that will run in that node. The smaller the image, the faster the node can download it.|", "<p>Start the application as quickly as possible . Some applications can take minutes to start| because of class loading, caching, and so on. When a Pod requires a long startup, your|", "<p>customers' requests might fail while your application is booting.|", "<p>Consider these two practices when designing your system, especially if you are expecting bursts or|", "<p>spikes. Having a small image and a fast startup helps you reduce scale-ups latency|", "", "<s2>\u00a0(#autoscaler_and_over-provisioning)", "<p>. Consequently, you can better handle tra\u00a8c increases without|", "<p>worrying too much about instability. These practices work better with the autoscaling best practices| discussed in GKE autoscaling", "<s2>\u00a0(#\u00a6ne-tune_gke_autoscaling)", "<p>.|", "<p>For more information about how to build containers, see Best practices for building containers|", "", "<s2>\u00a0(/solutions/best-practices-for-building-containers#build-the-smallest-image-possible)", "<p>.|", "", "<h3>Add Pod Disruption Budget to your application|", "", "<p>Pod Disruption Budget", "<s2> \u00a0(https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)", "<p> (PDB) limits the|", "<p>number of Pods that can be taken down simultaneously from a voluntary disruption|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 23/33|", "", "<s2>\u00a0(https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#voluntary-and-involuntary-disruptions)", "<p>.|", "<p>That means the de\u00a6ned disruption budget is respected at rollouts, node upgrades, and at any| autoscaling activities. However, this budget can not be guaranteed when involuntary things happen,|", "<p>such as hardware failure, kernel panic, or someone deleting a VM by mistake.|", "<p>When PDB is respected during the Cluster Autoscaler compacting phase, it's a best practice to| de\u00a6ne a Pod Disruption Budget for every application. This way you can control the minimum number|", "<p>of replicas required to support your load at any given time, including when CA is scaling down your| cluster.|", "<p>For more information, see Specifying a Disruption Budget for your Application|", "", "<s2>\u00a0(https://kubernetes.io/docs/tasks/run-application/con\u00a6gure-pdb/)", "<p>.|", "", "<h3>Set meaningful readiness and liveness probes for your application|", "", "<p>Setting meaningful probes ensures your application receives tra\u00a8c only when it is up and running|", "<p>and ready to accept tra\u00a8c. GKE uses readiness probes|", "", "<s2>\u00a0(https://kubernetes.io/docs/tasks/con\u00a6gure-pod-container/con\u00a6gure-liveness-readiness-startup-probes/#de\u00a6ne-| readiness-probes)|", "", "<p>to determine when to add Pods to or remove Pods from load balancers. GKE uses liveness probes|", "", "<s2>\u00a0(https://kubernetes.io/docs/tasks/con\u00a6gure-pod-container/con\u00a6gure-liveness-readiness-startup-probes/#de\u00a6ne-| a-liveness-command)|", "", "<p>to determine when to restart your Pods.|", "<p>The liveness probe is useful for telling Kubernetes that a given Pod is unable to make progress, for| example, when a deadlock state is detected. The readiness probe is useful for telling Kubernetes|", "<p>that your application isn't ready to receive tra\u00a8c, for example, while loading large cache data at| startup.|", "<p>To ensure the correct lifecycle of your application during scale-up activities, it's important to do the|", "<p>following:|", "<p>De\u00a6ne the readiness probe for all your containers.|", "<p>If your application depends on a cache to be loaded at startup, the readiness probe must say| it's ready only after the cache is fully loaded.|", "<p>If your application can start serving right away, a good default probe implementation can be as| simple as possible, for example, an HTTP endpoint returning a 200 status code.|", "<p>If you implement a more advanced probe, such as checking if the connection pool has|", "<p>available resources, make sure your error rate doesn't increase|", "", "<s2>\u00a0(#set_appropriate_resource_requests_and_limits)", "<p> as compared to a simpler implementation.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 24/33|", "", "<p>Never make any probe logic access other services. It can compromise the lifecycle of your Pod|", "<p>if these services don't respond promptly.|", "<p>For more information, see Con\u00a6gure Liveness, Readiness and Startup Probes|", "", "<s2>\u00a0(https://kubernetes.io/docs/tasks/con\u00a6gure-pod-container/con\u00a6gure-liveness-readiness-startup-probes/)", "<p>.|", "", "<h3>Make sure your applications are shu\u008fing down according to Kubernetes| expectations|", "", "<p>Autoscalers help you respond to spikes by spinning up new Pods and nodes, and by deleting them| when the spikes \u00a6nish. That means that to avoid errors while serving your Pods must be prepared|", "<p>for either a fast startup or a graceful shutdown.|", "<p>Because Kubernetes asynchronously updates endpoints and load balancers, it's important to follow| these best practices in order to ensure non-disruptive shutdowns:|", "<p>Don't stop accepting new requests right after ", "<s1>SIGTERM", "<p>. Your application must not stop|", "<p>immediately, but instead \u00a6nish all requests that are in \u00a7ight and still listen to incoming| connections that arrive after the Pod termination begins. It might take a while for Kubernetes|", "<p>to update all kube-proxies|", "", "<s2>\u00a0(https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/)", "<p> and load balancers.|", "<p>If your application terminates before these are updated, some requests might cause errors on| the client side.|", "<p>If your application doesn't follow the preceding practice, use the ", "<s1>preStop", "<p> hook|", "", "<s2>\u00a0(https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-details)", "<p>. Most| programs don't stop accepting requests right away. However, if you're using third-party code or|", "<p>are managing a system that you don't have control over, such as nginx", "<s2> \u00a0(https://www.nginx.com/)", "<p>,| the ", "<s1>preStop", "<p> hook is a good option for triggering a graceful shutdown without modifying the| application. One common strategy is to execute, in the ", "<s1>preStop", "<p> hook, a sleep of a few seconds|", "<p>to postpone the ", "<s1>SIGTERM", "<p>. This gives Kubernetes extra time to \u00a6nish the Pod deletion process,| and reduces connection errors on the client side.|", "<p>Handle SIGTERM for cleanups . If your application must clean up or has an in-memory state|", "<p>that must be persisted before the process terminates, now is the time to do it. Different| programming languages have different ways to catch this signal, so \u00a6nd the right way in your|", "<p>language.|", "<p>Con\u00a6gure ", "<s1>terminationGracePeriodSeconds|", "", "<s2>\u00a0(https://kubernetes.io/docs/tasks/con\u00a6gure-pod-container/con\u00a6gure-liveness-readiness-startup-probes)|", "", "<p>to \u00a6t your application needs . Some applications need more than the default 30 seconds to| \u00a6nish. In this case, you must specify ", "<s1>terminationGracePeriodSeconds", "<p>. High values might|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 25/33|", "", "<p>increase time for node upgrades or rollouts, for example. Low values might not allow enough|", "<p>time for Kubernetes to \u00a6nish the Pod termination process. Either way, we recommend that you| set your application's termination period to less than 10 minutes because Cluster Autoscaler| honors it for 10 minutes only", "<s2>\u00a0(/kubernetes-engine/docs/concepts/cluster-autoscaler#limitations)", "<p>.|", "<p>If your application uses container-native load balancing|", "", "<s2>\u00a0(#use_container-native_load_balancing_through_ingress)", "<p>, start failing your readiness probe when| you receive a SIGTERM . This action directly signals load balancers to stop forwarding new| requests to the backend Pod. Depending on the race between health check con\u00a6guration and|", "<p>endpoint programming, the backend Pod might be taken out of tra\u00a8c earlier.|", "<p>For more information, see Kubernetes best practices: terminating with grace|", "", "<s2>\u00a0(/blog/products/gcp/kubernetes-best-practices-terminating-with-grace)", "<p>.|", "", "<h3>Set up NodeLocal DNSCache|", "", "<p>The GKE-managed DNS is implemented by ", "<s1>kube-dns|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/service-discovery)", "<p>, an add-on deployed in all GKE clusters. When you| run DNS-hungry applications, the default ", "<s1>kube-dns-autoscaler", "<p> con\u00a6guration, which adjusts the| number of ", "<s1>kube-dns", "<p> replicas based on the number of nodes and cores in the cluster, might not be|", "<p>enough. In this scenario, DNS queries can either slow down or time out. To mitigate this problem,| companies are accustomed to tuning the ", "<s1>kube-dns-autoscaler", "<p> Con\u00a6gMap|", "", "<s2>\u00a0(https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/#tuning-autoscaling-| parameters)|", "", "<p>to increase the number of ", "<s1>kube-dns", "<p> replicas in their clusters. Although this strategy might work as|", "<p>expected, it increases the resource usage, and the total GKE cost.|", "<p>Another cost-optimized and more scalable alternative is to con\u00a6gure the NodeLocal DNSCache|", "", "<s2>\u00a0(/kubernetes-engine/docs/how-to/nodelocal-dns-cache)", "<p> in your cluster. NodeLocal DNSCache is an| optional GKE add-on that improves DNS lookup latency, makes DNS lookup times more consistent,| and reduces the number of DNS queries to ", "<s1>kube-dns", "<p> by running a DNS cache on each cluster node.|", "<p>For more information, see Setting up NodeLocal DNSCache|", "", "<s2>\u00a0(/kubernetes-engine/docs/how-to/nodelocal-dns-cache)", "<p>.|", "", "<h3>Use container-native load balancing through Ingress|", "", "<p>Container-native load balancing", "<s2>\u00a0(/kubernetes-engine/docs/concepts/container-native-load-balancing)", "<p> lets|", "<p>load balancers target Kubernetes Pods directly and to evenly distribute tra\u00a8c to Pods by using a| data model called network endpoint groups (NEGs)", "<s2>\u00a0(/load-balancing/docs/negs)", "<p>. This approach| improves network performance, increases visibility, enables advanced load-balancing features, and|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 26/33|", "", "<p>enables the use of Tra\u00a8c Director", "<s2>\u00a0(/tra\u00a8c-director/docs)", "<p>, Google Cloud's fully managed tra\u00a8c control|", "<p>plane for service mesh.|", "<p>Because of these bene\u00a6ts, container-native load balancing is the recommended solution for load| balancing through Ingress", "<s2>\u00a0(/kubernetes-engine/docs/how-to/container-native-load-balancing)", "<p>. When NEGs| are used with GKE Ingress, the Ingress controller facilitates the creation of all aspects of the L7 load| balancer. This includes creating the virtual IP address, forwarding rules, health checks, \u00a6rewall rules,|", "<p>and more.|", "<p>Container-native load balancing becomes even more important when using Cluster Autoscaler. For| non-NEG load balancers, during scale downs, load-balancing programming, and connection draining| might not be fully completed before Cluster Autoscaler terminates the node instances. This might| disrupt ongoing connections \u00a7owing through the node even when the backend Pods are not on the|", "<p>node.|", "<p>Container-native load balancing is enabled by default for Services when all of the following| conditions are true:|", "<p>The Services were created in GKE clusters 1.17.6-gke.7 and higher.|", "<p>If you are using VPC-native clusters.|", "<p>If you are not using a Shared VPC.|", "<p>If you are not using GKE Network Policy.|", "<p>For more information, see Ingress GKE documentation|", "", "<s2>\u00a0(/kubernetes-engine/docs/concepts/ingress#container-native_load_balancing)", "<p> and Using container-native| load balancing", "<s2>\u00a0(/kubernetes-engine/docs/how-to/container-native-load-balancing)", "<p>.|", "", "<h3>Consider using retries with exponential backo\u0080|", "", "<p>In microservices architectures running on Kubernetes, transient failures might occur for various|", "<p>reasons\u2014for example:|", "<p>A large spike that triggered a still-working scale-up|", "<p>Network failures|", "<p>Connections dropped due to Pods not shutting down|", "", "<s2>\u00a0(#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectations)|", "", "<p>Preemptible VMs shutting down inadvertently|", "", "<s2>\u00a0(#prepare_your_environment_to_\u00a6t_your_workload_type)|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 27/33|", "", "<p>Applications reaching their rating limits|", "<p>These issues are ephemeral, and you can mitigate them by calling the service again after a delay.|", "<p>However, to prevent overwhelming the destination service with requests, it's important that you| execute these calls using an exponential backoff.|", "<p>To facilitate such a retry pattern, many existing libraries implement the exponential retrial logic. You| can use your library of choice or write your own code. If you use Istio|", "", "<s2>\u00a0(https://istio.io/latest/docs/concepts/what-is-istio/)", "<p> or Anthos Service Mesh", "<s2>\u00a0(/anthos/service-mesh)", "<p> (ASM),|", "<p>you can opt for the proxy-level retry", "<s2> \u00a0(https://istio.io/latest/docs/concepts/tra\u00a8c-management/#retries)|", "", "<p>mechanism, which transparently executes retries on your behalf.|", "<p>It's important to plan for your application to support service call retries, for example, to avoid| inserting already-inserted information. Consider that a chain of retries might impact the latency of| your \u00a6nal user, which might time-out if not correctly planned.|", "", "<h2>Monitor your environment and enforce cost-optimized con\u0089guratio| and practices|", "", "<p>In many medium and large enterprises, a centralized platform and infrastructure team is often| responsible for creating, maintaining, and monitoring Kubernetes clusters for the entire company.| This represents a strong need for having resource usage accountability and for making sure all| teams are following the company's policies. This section addresses options for monitoring and|", "<p>enforcing cost-related practices.|", "", "<h3>Observe your GKE clusters and watch for recommendations|", "", "<p>You can check the resource utilization in a Kubernetes cluster by examining the containers, Pods,|", "<p>and services, and the characteristics of the overall cluster. There are many ways you can perform| this task, but the initial approach we recommend is observing your GKE clusters through the| Monitoring Dashboard", "<s2>\u00a0(/stackdriver/docs/solutions/kubernetes-engine/observing)", "<p>. This gives you time-| series data of how your cluster is being used, letting you aggregate and span from infrastructure,|", "<p>workloads, and services.|", "<p>Although this is a good starting point, Google Cloud provides other options\u2014for example:|", "<p>In the Google Cloud console, on the  GKE Clusters  page, look at the  Noti\u00a6cations  column. If you| have high resource waste in a cluster, the UI gives you a hint of the overall allocated versus| requested information.|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 28/33|", "", "<s2>Go to GKE Cluster List \u00a0(https://console.cloud.google.com/kubernetes/list)|", "", "<p>In the Google Cloud console, on the  Recommendations  page, look for  Cost savings| recommendation cards.|", "", "<s2>Go to Recommendation Hub \u00a0(https://console.cloud.google.com/home/recommendations)|", "", "<p>For more details, see Observing your GKE clusters|", "", "<s2>\u00a0(/stackdriver/docs/solutions/kubernetes-engine/observing)", "<p> and Getting started with Recommendation Hub|", "", "<s2>\u00a0(/recommender/docs/recommendation-hub/identify-con\u00a6guration-problems)", "<p>.|", "", "<h3>Enable GKE usage metering|", "", "<p>For a more \u00a7exible approach that lets you see approximate cost breakdowns, try GKE usage| metering", "<s2>\u00a0(/kubernetes-engine/docs/how-to/cluster-usage-metering)", "<p>. GKE usage metering lets you see your| GKE clusters' usage pro\u00a6les broken down by namespaces and labels. It tracks information about the| resource requests and resource consumption of your cluster's workloads, such as CPU, GPU, TPU,|", "<p>memory, storage, and optionally network egress.|", "<p>GKE usage metering helps you understand the overall cost structure of your GKE clusters, what team| or application is spending the most, which environment or component caused a sudden spike in| usage or costs, and which team is being wasteful. By comparing resource requests with actual| utilization, you can understand which workloads are either under- or over-provisioned.|", "<p>You can take advantage of the default Looker Studio templates, or go a step further and customize|", "<p>the dashboards according to your organizational needs. For more information about GKE usage| metering and its prerequisites, see Understanding cluster resource usage|", "", "<s2>\u00a0(/kubernetes-engine/docs/how-to/cluster-usage-metering)", "<p>.|", "", "<h3>Understand how Metrics Server works and monitor it|", "", "<p>Metrics Server is the source of the container resource metrics for GKE built-in autoscaling pipelines.| Metrics Server retrieves metrics from kubelets|", "", "<s2>\u00a0(https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)", "<p> and exposes them through|", "<p>the Kubernetes Metrics API", "<s2> \u00a0(https://github.com/kubernetes/metrics)", "<p>. HPA and VPA then use these| metrics to determine when to trigger autoscaling.|", "<p>For the health of GKE autoscaling, you must have a healthy Metrics Server. With GKE ", "<s1>metrics-|", "<s1>server", "<p> deployment, a resizer  nanny  is installed, which makes the Metrics Server container grow| vertically by adding or removing CPU and memory according to the cluster's node count. In-place|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 29/33|", "", "<p>update of Pods is still not supported in Kubernetes, which is why the nanny must restart the|", "", "<s1>metrics-server", "<p> Pod to apply the new required resources.|", "<p>Although the restart happens quickly, the total latency for autoscalers to realize they must act can be|", "<p>slightly increased after a ", "<s1>metrics-server", "<p> resize. To avoid Metrics Server frequent restarts in fast-| changing clusters, starting at GKE 1.15.11-gke.9, the nanny supports resize delays|", "", "<s2>\u00a0(https://github.com/kubernetes/autoscaler/issues/2597)", "<p>.|", "<p>Follow these best practices when using Metric Server:|", "<p>Pick the GKE version that supports ", "<s1>metrics-server", "<p> resize delays. You can con\u00a6rm it by| checking whether the ", "<s1>metrics-server", "<p> deployment YAML \u00a6le has the ", "<s1>scale-down-delay|", "", "<p>con\u00a6guration in the ", "<s1>metrics-server-nanny", "<p> container.|", "<p>Monitor ", "<s1>metrics-server", "<p> deployment. If Metrics Server is down, it means no autoscaling is| working at all. You want your top-priority monitoring services to monitor this deployment.|", "<p>Follow the best practices discussed in GKE autoscaling", "<s2>\u00a0(#\u00a6ne-tune_gke_autoscaling)", "<p>.|", "", "<h3>Use Kubernetes Resource Quotas|", "", "<p>In multi-tenant clusters, different teams commonly become responsible for applications deployed in|", "<p>different namespaces. For a centralized platform and infrastructure group, it's a concern that one| team might use more resources than necessary. Starving all cluster's compute resources or even| triggering too many scale-ups can increase your costs.|", "<p>To address this concern, you must use resource quotas|", "", "<s2>\u00a0(https://kubernetes.io/docs/concepts/policy/resource-quotas/)", "<p>. Resource quotas manage the amount of|", "<p>resources used by objects in a namespace. You can set quotas in terms of compute (CPU and| memory) and storage resources, or in terms of object counts. Resource quotas let you ensure that| no tenant uses more than its assigned share of cluster resources.|", "<p>For more information, see Con\u00a6gure Memory and CPU Quotas for a Namespace|", "", "<s2>\u00a0(https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)", "<p>.|", "", "<h3>Consider using GKE Enterprise Policy Controller|", "", "<p>GKE Enterprise Policy Controller", "<s2>\u00a0(/anthos-con\u00a6g-management/docs/concepts/policy-controller)", "<p> (APC) is a| Kubernetes dynamic admission controller|", "", "<s2>\u00a0(https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/)", "<p> that checks,| audits, and enforces your clusters' compliance with policies related to security, regulations, or| arbitrary business rules. Policy Controller uses constraints to enforce your clusters' compliance. For|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 30/33|", "", "<p>example, you can install in your cluster constraints for many of the best practices discussed in the| Preparing your cloud-based Kubernetes application", "<s2>\u00a0(#prepare_your_cloud-based_kubernetes_application)|", "<p>section. This way, deployments are rejected if they don't strictly adhere to your Kubernetes practices.|", "<p>Enforcing such rules helps to avoid unexpected cost spikes and reduces the chances of having| workload instability during autoscaling.|", "<p>For more information about how to enforce and write your own rules, see Creating constraints|", "", "<s2>\u00a0(/anthos-con\u00a6g-management/docs/how-to/creating-policy-controller-constraints)", "<p> and Writing a constraint| template", "<s2>\u00a0(/anthos-con\u00a6g-management/docs/how-to/write-custom-constraint-templates)", "<p>. If you are not an|", "<p>GKE Enterprise", "<s2>\u00a0(/anthos)", "<p> customer, you can consider using Gatekeeper|", "", "<s2>\u00a0(https://github.com/open-policy-agent/gatekeeper)", "<p>, the open source software that APC is built on.|", "", "<h3>Design your CI/CD pipeline to enforce cost-saving practices|", "", "<p>GKE Enterprise Policy Controller helps you avoid deploying noncompliant software in your GKE| cluster. However, we recommend that you enforce such policy constraints early in your development| cycle, whether in pre-commit checks, pull request|", "", "<s2>\u00a0(https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests)", "<p> checks,|", "<p>delivery work\u00a7ows, or any step that makes sense in your environment. This practice lets you \u00a6nd and| \u00a6x miscon\u00a6gurations quickly, and helps you understand what you need to pay attention to by| creating guardrails.|", "<p>Also consider using kpt functions", "<s2> \u00a0(https://googlecontainertools.github.io/kpt/reference/fn/)", "<p> in your CI/CD| pipeline to validate whether your Kubernetes con\u00a6guration \u00a6les adhere to the constraints enforced|", "<p>by GKE Enterprise Policy Controller, and to estimate resource utilization or deployment cost. This| way, you can stop the pipeline when a cost-related issue is detected. Or you can create a different| deployment approval process for con\u00a6gurations that, for example, increase the number of replicas.|", "<p>For more information, see Using Policy Controller in a CI pipeline|", "", "<s2>\u00a0(/anthos-con\u00a6g-management/docs/tutorials/app-policy-validation-ci-pipeline)", "<p>, and for a complete example of|", "<p>a delivery platform, see Modern CI/CD with GKE Enterprise|", "", "<s2>\u00a0(https://github.com/GoogleCloudPlatform/solutions-modern-cicd-anthos)", "<p>.|", "", "<h2>Spread the cost saving culture|", "", "<p>Many organizations create abstractions and platforms to hide infrastructure complexity from you.| This is a common practice in companies that are migrating their services from virtual machines to| Kubernetes. Sometimes these companies let developers con\u00a6gure their own applications in|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 31/33|", "", "<p>production. However, it's not uncommon to see developers who have never touched a Kubernetes| cluster.|", "<p>The practices we recommend in this section don't mean that you should stop doing abstractions at| all. Instead, they help you view your spending on Google Cloud and train your developers and|", "<p>operators on your infrastructure. You can do this by creating learning incentives and programs where| you can use traditional or online classes, discussion groups, peer reviews, pair programming, CI/CD| and cost-saving gami\u00a6cations, and more. For example, in the Kubernetes world, it's important to| understand the impact of a 3 Gb image application, a missing readiness probe, or an HPA|", "<p>miscon\u00a6guration.|", "<p>Finally, as shown in Google's DORA research", "<s2>\u00a0(/devops)", "<p>, culture capabilities are some of the main| factors that drive better organizational performance, less rework, less burnout, and so on. Cost| saving is no different. Giving your employees access to their spending aligns them more closely with| business objectives and constraints.|", "", "<h2>Summary of best practices|", "", "<p>The following table summarizes the best practices recommended in this document.|", "", "<s2>Topic| Task|", "<s2>GKE cost-| optimization| features and| options|", "<s2>Fine-tune GKE autoscaling\u00a0(#\u00a6ne-tune_gke_autoscaling)|", "<s2>Choose the right machine type\u00a0(#choose_the_right_machine_type)|", "<s2>Select the appropriate region\u00a0(#select_the_appropriate_region)|", "<s2>Sign up for committed-use discounts\u00a0(#sign_up_for_committed_use_discounts)|", "<s2>Review small development clusters\u00a0(#review_small_development_clusters)|", "<s2>Review your logging and monitoring strategies|", "<s2>\u00a0(#review_your_logging_and_monitoring_strategies)|", "<s2>Review inter-region egress tra\u00a8c in regional and multi-zonal clusters|", "<s2>\u00a0(#review_inter-region_egress_tra\u00a8c_in_regional_and_multi-zonal_clusters)|", "<s2>Prepare your environment to \u00a6t your workload type|", "<s2>\u00a0(#prepare_your_environment_to_\u00a6t_your_workload_type)|", "<s2>Prepare your| cloud-native| Kubernetes| applications|", "<s2>Understand your application capacity\u00a0(#understand_your_application_capacity)|", "<s2>Make sure your application can grow both vertically and horizontally|", "<s2>\u00a0(#make_sure_your_application_can_grow_vertically_and_horizontally)|", "", "<h2>\ue835| \ue835| \ue835| \ue835| \ue835| \ue835|", "<h2>\ue835|", "<h2>\ue835|", "<h2>\ue835| \ue835|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 32/33|", "", "<s2>Topic| Task|", "<s2>Set appropriate resource requests and limits\u00a0(#set_appropriate_resource_requests_and_limits|", "<s2>Make sure your container is as lean as possible|", "<s2>\u00a0(#make_sure_your_container_is_as_lean_as_possible)|", "<s2>Add Pod Disruption Budget to your application|", "<s2>\u00a0(#add-pod_disruption_budget-to-your-application)|", "<s2>Set meaningful readiness and liveness probes for your application|", "<s2>\u00a0(#set_meaningful_readiness_and_liveness_probes_for_your_application)|", "<s2>Make sure your applications are shutting down in accordance with Kubernetes expectations|", "<s2>\u00a0(#make_sure_your_applications_are_shutting_down_in_accordance_with_kubernetes_expectation|", "<s2>Setup NodeLocal DNSCache\u00a0(#set_up_nodelocal_dnscache)|", "<s2>Use container-native load balancing through Ingress|", "<s2>\u00a0(#use_container-native_load_balancing_through_ingress)|", "<s2>Consider using retries with exponential backoff|", "<s2>\u00a0(#consider_using_retries_with_exponential_backoff)|", "<s2>Monitor your| environment| and enforce| cost-| optimized| con\u00a6gurations| and practices|", "<s2>Observe your GKE clusters and watch for recommendations\u00a0(#enable_gke_usage_metering)|", "<s2>Enable GKE usage metering\u00a0(#enable_gke_usage_metering)|", "<s2>Understand how Metrics Server works and monitor it|", "<s2>\u00a0(#understand_how_metrics_server_works_and_how_to_monitor_it)|", "<s2>Use Kubernetes Resource Quotas\u00a0(#use_kubernetes_resource_quotas)|", "<s2>Consider using GKE Enterprise Policy Controller\u00a0(#spread_the_cost-saving_culture)|", "<s2>Design your CI/CD pipeline to enforce cost-saving practices|", "<s2>\u00a0(#design_your_cicd_pipeline_to_enforce_cost-saving_practices)|", "<s2>Culture| Spread the cost-saving culture\u00a0(#spread_the_cost-saving_culture)|", "", "<h2>What's next|", "", "<p>Find more tips and best practices for optimizing costs at Cost optimization on Google Cloud|", "<p>for developers and operators", "<s2>\u00a0(/solutions/cost-e\u00a8ciency-on-google-cloud)", "<p>.|", "<p>For more details on how to lower costs on batch applications, see Optimizing resource usage| in a multi-tenant GKE cluster using node auto-provisioning|", "", "<s2>\u00a0(/solutions/optimizing-resources-in-multi-tenant-gke-clusters-with-auto-provisioning)", "<p>.|", "", "<h2>\ue835| \ue835|", "<h2>\ue835|", "<h2>\ue835|", "<h2>\ue835|", "<h2>\ue835| \ue835|", "<h2>\ue835|", "<h2>\ue835| \ue835| \ue835|", "<h2>\ue835| \ue835| \ue835|", "<h2>\ue835|", "", "<s4>2/2/24, 9:59 PM| Best practices for running cost-optimized Kubernetes applications on GKE \u00a0|\u00a0 Cloud Architecture Center \u00a0|\u00a0 Google Cloud|", "<s4>https://cloud.google.com/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke| 33/33|", "", "<p>To learn how to save money at night or at other times when usage is lower, see the Reducing|", "<p>costs by scaling down GKE clusters during off-peak hours|", "", "<s2>\u00a0(/solutions/reducing-costs-by-scaling-down-gke-off-hours)", "<p> tutorial.|", "<p>To learn more about using Spot VMs, see the Run web applications on GKE using cost-|", "<p>optimized Spot VMs|", "", "<s2>\u00a0(/kubernetes-engine/docs/archive/run-web-applications-on-gke-using-cost-optimized-spot-vms-and-tra\u00a8c-| director)|", "<p>tutorial.|", "<p>To understand how you can save money on logging and monitoring, take a look at Cost| optimization for Cloud Logging, Cloud Monitoring, and Application Performance Management|", "", "<s2>\u00a0(/solutions/stackdriver-cost-optimization)", "<p>.|", "<p>For reducing costs in Google Cloud in general, see Understanding the principles of cost| optimization", "<s2>\u00a0(/resources/principles-of-cost-optimization-whitepaper)", "<p>.|", "<p>For a broader discussion of scalability, see Patterns for scalable and resilient apps|", "", "<s2>\u00a0(/solutions/scalable-and-resilient-apps)", "<p>.|", "<p>Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look| at our Cloud Architecture Center", "<s2>\u00a0(/architecture)", "<p>.|", "<p>Learn more about Running a GKE application on Spot VMs with on-demand nodes as fallback|", "", "<s2>\u00a0(/blog/topics/developers-practitioners/running-gke-application-spot-nodes-demand-nodes-fallback)", "<p>.|", "", "<s3>Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License|", "", "<s2>\u00a0(https://creativecommons.org/licenses/by/4.0/)", "<s3>, and code samples are licensed under the Apache 2.0 License|", "", "<s2>\u00a0(https://www.apache.org/licenses/LICENSE-2.0)", "<s3>. For details, see the Google Developers Site Policies|", "", "<s2>\u00a0(https://developers.google.com/site-policies)", "<s3>. Java is a registered trademark of Oracle and/or its a\u00a8liates.|", "<s3>Last updated 2024-01-31 UTC.|"]